{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Turing Pi Documentation Welcome the Turing Pi Community, let's get you started . Hardware Software Full Documentation Turing Pi Cluster Board Specifications Cluster Management Bus Kubernetes Helm & Tiller Tiller Wiki Use Cases Have you mastered your very own Turing Pi? Consider contributing to this wiki. Posting your build on the #use-cases Discord Channel or reach out to the #wiki-squad with suggestions. Do you have feedback for making this Wiki better? We appreciate your feedback, please reach out to us on the Discord Channel #wiki-squad .","title":"Home"},{"location":"#turing-pi","text":"","title":"Turing Pi"},{"location":"#documentation","text":"Welcome the Turing Pi Community, let's get you started . Hardware Software","title":"Documentation"},{"location":"#full-documentation","text":"Turing Pi Cluster Board Specifications Cluster Management Bus Kubernetes Helm & Tiller Tiller","title":"Full Documentation"},{"location":"#wiki","text":"Use Cases","title":"Wiki"},{"location":"#have-you-mastered-your-very-own-turing-pi","text":"Consider contributing to this wiki. Posting your build on the #use-cases Discord Channel or reach out to the #wiki-squad with suggestions.","title":"Have you mastered your very own Turing Pi?"},{"location":"#do-you-have-feedback-for-making-this-wiki-better","text":"We appreciate your feedback, please reach out to us on the Discord Channel #wiki-squad .","title":"Do you have feedback for making this Wiki better?"},{"location":"gettingstarted/","text":"Getting Started this outlines the basic process of getting your TuringPi up and running. Hardware Software Full Documentation Discord Channel #tech-support","title":"Getting Started"},{"location":"gettingstarted/#getting-started","text":"this outlines the basic process of getting your TuringPi up and running. Hardware Software Full Documentation Discord Channel #tech-support","title":"Getting Started"},{"location":"FAQ/","text":"FAQ Here you find a list with the most frequent asked questions What can I do with the board? Home server (homelab) and application hosting Develop and learn cloud-native technologies (Kubernetes, Docker Swarm, Serverless, Microservices) on bare metal Cloud-native apps testing environment Learn concepts of distributed Machine Learning apps Hosting and managing IoT apps Prototype and learn cluster applications, parallel computing, and distributed computing concepts on bare metal Host K8S, K3S, Minecraft, Plex, Owncloud, Nextcloud, Seafile, Minio, Tensorflow, \u2026 Which Raspberry Pi models are compatible? Turing Pi supports the following models with and without eMMC Raspberry Pi Compute Module 1 Raspberry Pi Compute Module 3 Raspberry Pi Compute Module 3+ Does the board support the new Rasberry pi 4? There is no Compute Model for the Raspberry pi 4th generation yet How to the compute modules communicate with each other? The nodes interconnected with the onboard 1 Gbps switch. However, each node is limited with 100 Mbps USB speed. Also, there is an I2C bus to exchange some technical information between nodes, including Real-Time Clock (RTC). From where the cluster board boots OS? You can boot the OS either from eMMC, SD card or netboot Does each node get its own IP address? Yes Can I flash compute modules through the board? Yes, you can flash a compute module using a top/master node. How do the Ethernet, USB, HDMI, and audio ports work? Is there a way to switch them between nodes? There are 8 USB on the board. Each pair of USB connected to a particular node. 2x USB routed to the top/master node, 2x to the second node, 2x to the fourth node, 2x to the 6th node. HDMI and audio connected with a top/master node. There are ATX and DC 12V power ports on the cluster board. Does this mean it can function from either an ATX power supply or 12V? Yes","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#here-you-find-a-list-with-the-most-frequent-asked-questions","text":"","title":"Here you find a list with the most frequent asked questions"},{"location":"FAQ/#what-can-i-do-with-the-board","text":"Home server (homelab) and application hosting Develop and learn cloud-native technologies (Kubernetes, Docker Swarm, Serverless, Microservices) on bare metal Cloud-native apps testing environment Learn concepts of distributed Machine Learning apps Hosting and managing IoT apps Prototype and learn cluster applications, parallel computing, and distributed computing concepts on bare metal Host K8S, K3S, Minecraft, Plex, Owncloud, Nextcloud, Seafile, Minio, Tensorflow, \u2026","title":"What can I do with the board?"},{"location":"FAQ/#which-raspberry-pi-models-are-compatible","text":"Turing Pi supports the following models with and without eMMC Raspberry Pi Compute Module 1 Raspberry Pi Compute Module 3 Raspberry Pi Compute Module 3+","title":"Which Raspberry Pi models are compatible?"},{"location":"FAQ/#does-the-board-support-the-new-rasberry-pi-4","text":"There is no Compute Model for the Raspberry pi 4th generation yet","title":"Does the board support the new Rasberry pi 4?"},{"location":"FAQ/#how-to-the-compute-modules-communicate-with-each-other","text":"The nodes interconnected with the onboard 1 Gbps switch. However, each node is limited with 100 Mbps USB speed. Also, there is an I2C bus to exchange some technical information between nodes, including Real-Time Clock (RTC).","title":"How to the compute modules communicate with each other?"},{"location":"FAQ/#from-where-the-cluster-board-boots-os","text":"You can boot the OS either from eMMC, SD card or netboot","title":"From where the cluster board boots OS?"},{"location":"FAQ/#does-each-node-get-its-own-ip-address","text":"Yes","title":"Does each node get its own IP address?"},{"location":"FAQ/#can-i-flash-compute-modules-through-the-board","text":"Yes, you can flash a compute module using a top/master node.","title":"Can I flash compute modules through the board?"},{"location":"FAQ/#how-do-the-ethernet-usb-hdmi-and-audio-ports-work-is-there-a-way-to-switch-them-between-nodes","text":"There are 8 USB on the board. Each pair of USB connected to a particular node. 2x USB routed to the top/master node, 2x to the second node, 2x to the fourth node, 2x to the 6th node. HDMI and audio connected with a top/master node.","title":"How do the Ethernet, USB, HDMI, and audio ports work? Is there a way to switch them between nodes?"},{"location":"FAQ/#there-are-atx-and-dc-12v-power-ports-on-the-cluster-board-does-this-mean-it-can-function-from-either-an-atx-power-supply-or-12v","text":"Yes","title":"There are ATX and DC 12V power ports on the cluster board. Does this mean it can function from either an ATX power supply or 12V?"},{"location":"Kubernetes/AddWorker/","text":"Add a worker During some of the manipulation of the partition table of my SD card, I ended up screwing up both my SD card and my backup Win32DiskImage backup. Moreover if your SD card is 32G, it takes around 30 minute to restore from backup. Hence the idea to come up with a way to build more resiliency in the cluster. Recreating a node from scratch should not take more than 10 mn. The propose procedure is still rather long because I did not push enough yet what the HypriotOS team, aka build a default SD image where cloud-init does 100% of the initialization work. Base OS Flash HypriotOS to SD and reboot Pi. OS Flash the SD Card with HypriotOS Connect Pi through to the cluster switch Freeze the new node IP Access the Master PI, run arp -a and find the new IP. Freeze the new IP in the dhcpcd.conf ssh <masterip> arp -a vi /etc/dhcp/dhcpd.conf Ssh to the new node Access the node from the Master PI: ssh 192.168.2.xxx docker ps Basic hostname Freeze your configuration sudo apt-get remove --purge cloud-init sudo apt-get autoremove Set the host name sudo vi /etc/hosts sudo hostnamectl set-hostname <xxx> Install kubeadm On Master PI Firt access the kubemaster node and regenerate a token (if you did not use ttl=0 when using kubeadm init on the master): kubeadm token create On New Worker PI sudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubeadm kubectl kubelet To help during the initialization phase, get kubeadm to download the images onto docker kubeadm config images pull Get the node to join the cluster kubeadm join 192.168.2.1:6443 --token yyyyyy.xxxx --discovery-token-ca-cert-hash sha256:zzzz Conclusion Will have to come back later and use cloud-init, create a clean & small SD image for Win32DiskImage Will have to create more advanced partition on the SD card.","title":"Add a worker"},{"location":"Kubernetes/AddWorker/#add-a-worker","text":"During some of the manipulation of the partition table of my SD card, I ended up screwing up both my SD card and my backup Win32DiskImage backup. Moreover if your SD card is 32G, it takes around 30 minute to restore from backup. Hence the idea to come up with a way to build more resiliency in the cluster. Recreating a node from scratch should not take more than 10 mn. The propose procedure is still rather long because I did not push enough yet what the HypriotOS team, aka build a default SD image where cloud-init does 100% of the initialization work.","title":"Add a worker"},{"location":"Kubernetes/AddWorker/#base-os","text":"Flash HypriotOS to SD and reboot Pi.","title":"Base OS"},{"location":"Kubernetes/AddWorker/#os","text":"Flash the SD Card with HypriotOS Connect Pi through to the cluster switch","title":"OS"},{"location":"Kubernetes/AddWorker/#freeze-the-new-node-ip","text":"Access the Master PI, run arp -a and find the new IP. Freeze the new IP in the dhcpcd.conf ssh <masterip> arp -a vi /etc/dhcp/dhcpd.conf","title":"Freeze the new node IP"},{"location":"Kubernetes/AddWorker/#ssh-to-the-new-node","text":"Access the node from the Master PI: ssh 192.168.2.xxx docker ps","title":"Ssh to the new node"},{"location":"Kubernetes/AddWorker/#basic-hostname","text":"Freeze your configuration sudo apt-get remove --purge cloud-init sudo apt-get autoremove Set the host name sudo vi /etc/hosts sudo hostnamectl set-hostname <xxx>","title":"Basic hostname"},{"location":"Kubernetes/AddWorker/#install-kubeadm","text":"","title":"Install kubeadm"},{"location":"Kubernetes/AddWorker/#on-master-pi","text":"Firt access the kubemaster node and regenerate a token (if you did not use ttl=0 when using kubeadm init on the master): kubeadm token create","title":"On Master PI"},{"location":"Kubernetes/AddWorker/#on-new-worker-pi","text":"sudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubeadm kubectl kubelet To help during the initialization phase, get kubeadm to download the images onto docker kubeadm config images pull Get the node to join the cluster kubeadm join 192.168.2.1:6443 --token yyyyyy.xxxx --discovery-token-ca-cert-hash sha256:zzzz","title":"On New Worker PI"},{"location":"Kubernetes/AddWorker/#conclusion","text":"Will have to come back later and use cloud-init, create a clean & small SD image for Win32DiskImage Will have to create more advanced partition on the SD card.","title":"Conclusion"},{"location":"Kubernetes/DeployFlannel/","text":"Deploy Flannel in Turing Pi In order to get the nodes and pods interface with each other accross the cluster. This post describes how I deployed Flannel acounting with the fact that some of the nodes have multiple interfaces (wlan0 and eth0). Key Aspects Flannel seems to deploy ok. Looks like in trouble when multiple interfaces available Calico in not compiled by default for Rapsberry PI Flannel Setup through kubectl The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment cd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd kube-deployment/flannel/ kubectl apply -f flannel.yaml ### Flannel Issue 1: flannel.1. Link has incompatible address on master-pi, both the WLAN and LAN interfaces were activated. After unplugging the CAT5, behavior was similar. Moreover this had some impact on the kube-apiserver (see the number of restarts). ```bash $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m 1/1 Running 0 4h kube-system pod/coredns-78fcdf6894-cw5p8 1/1 Running 15 10d kube-system pod/coredns-78fcdf6894-czjcj 1/1 Running 15 10d kube-system pod/etcd-master-pi 1/1 Running 11 10d kube-system pod/kube-apiserver-master-pi 1/1 Running 599 10d kube-system pod/kube-controller-manager-master-pi 1/1 Running 38 10d kube-system pod/kube-flannel-ds-bhllh 1/1 Running 13 9d kube-system pod/kube-flannel-ds-q7cp2 0/1 CrashLoopBackOff 401 9d kube-system pod/kube-flannel-ds-wqxsz 1/1 Running 16 9d kube-system pod/kube-proxy-4chwh 1/1 Running 9 9d kube-system pod/kube-proxy-6r5mn 1/1 Running 5 9d kube-system pod/kube-proxy-vvj6j 1/1 Running 11 10d kube-system pod/kube-scheduler-master-pi 1/1 Running 13 10d kube-system pod/kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 20 7d kube-system pod/tiller-deploy-b59fcc885-66l7s 1/1 Running 0 6h $ kubectl logs pod/kube-flannel-ds-q7cp2 -n kube-system I0716 00:42:46.596796 1 main.go:474] Determining IP address of default interface I0716 00:42:46.598043 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:42:46.598138 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:42:46.775936 1 kube.go:283] Starting kube subnet manager I0716 00:42:46.775907 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:42:47.776280 1 kube.go:137] Node controller sync successful I0716 00:42:47.776400 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:42:47.776431 1 main.go:237] Installing signal handlers I0716 00:42:47.776697 1 main.go:352] Found network config - Backend type: vxlan I0716 00:42:47.776900 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0716 00:42:47.778884 1 main.go:279] Error registering network: failed to configure interface flannel.1: link has incompatible addresses. Remove additional addresses and try again.... I0716 00:42:47.778991 1 main.go:332] Stopping shutdownHandler... Deleting the pod, did not help. After recreation same issue reappeared. $ kubectl delete pod/kube-flannel-ds-q7cp2 -n kube-system $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 0/1 Error 1 17s Deleting the interface flannel.1 interface actually worked: $ sudo ip link delete flannel.1 $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 1/1 Running 5 3m $ kubectl logs pod/kube-flannel-ds-z7w4f -n kube-system I0716 00:52:14.555290 1 main.go:474] Determining IP address of default interface I0716 00:52:14.564490 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:52:14.564578 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:52:14.802491 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:52:14.802544 1 kube.go:283] Starting kube subnet manager I0716 00:52:15.803114 1 kube.go:137] Node controller sync successful I0716 00:52:15.803308 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:52:15.803909 1 main.go:237] Installing signal handlers I0716 00:52:15.804662 1 main.go:352] Found network config - Backend type: vxlan I0716 00:52:15.804985 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0716 00:52:15.875242 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0716 00:52:15.875367 1 main.go:303] Running backend. I0716 00:52:15.875489 1 main.go:321] Waiting for all goroutines to exit I0716 00:52:15.875559 1 vxlan_network.go:56] watching for new subnet leases Flannel issue 2: Multiple interfaces Some of the PI have two interfaces running: wlan0 and eth0. The internal cluster network is using eth0. We need to force Flannel to use it. Setup through kubectl Realize I was using v0.9.1 instead of v0.10.0. Let's update the file $ mkdir -p $HOME/kube-deployments/flannel $ cd $HOME/kube-deployments/flannel $ curl -sSL https://rawgit.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml | sed \"s/amd64/arm/g\" > flannel.yaml Let's add --iface=eth0 to the flanneld to in the flannel.yaml Let's update flannel from 0.9.1 to 0.10.0 at the same time we specify which interface to use. $ kubectl apply -f flannel.yaml clusterrole.rbac.authorization.k8s.io/flannel configured clusterrolebinding.rbac.authorization.k8s.io/flannel configured serviceaccount/flannel unchanged configmap/kube-flannel-cfg configured daemonset.extensions/kube-flannel-ds configured It seems it solved the flannel issue. The bug in kube 1.11.0 still there (restart of kube-apiserver) Will update to 1.11.1 when it is published $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-78fcdf6894-bn6wl 1/1 Running 0 6d kube-system pod/coredns-78fcdf6894-k52xb 1/1 Running 0 6d kube-system pod/etcd-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-controller-manager-kubemaster-pi 0/1 CrashLoopBackOff 1740 6d kube-system pod/kube-flannel-ds-62fz9 1/1 Running 984 6d kube-system pod/kube-flannel-ds-gwzdt 1/1 Running 0 6d kube-system pod/kube-flannel-ds-h7ln5 1/1 Running 0 6d kube-system pod/kube-flannel-ds-qs9lf 1/1 Running 0 6d kube-system pod/kube-flannel-ds-vwsjk 1/1 Running 0 6d kube-system pod/kube-proxy-45z5s 1/1 Running 0 6d kube-system pod/kube-proxy-4trsd 1/1 Running 0 6d kube-system pod/kube-proxy-ksj7c 1/1 Running 4 6d kube-system pod/kube-proxy-t7gmc 1/1 Running 0 6d kube-system pod/kube-proxy-tfmqb 1/1 Running 0 6d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 4 6d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 6d kube-system service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP 6d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 2 2 2 2 6d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-78fcdf6894 2 2 2 6d $ kubectl logs pod/kube-flannel-ds-62fz9 -n kube-system I0714 14:34:26.719081 1 main.go:474] Determining IP address of default interface I0714 14:34:26.728184 1 main.go:487] Using interface with name wlan0 and address 192.168.1.94 I0714 14:34:26.728273 1 main.go:504] Defaulting external address to interface address (192.168.1.94) I0714 14:34:26.942686 1 kube.go:283] Starting kube subnet manager I0714 14:34:26.943203 1 kube.go:130] Waiting 10m0s for node controller to sync I0714 14:34:27.943672 1 kube.go:137] Node controller sync successful I0714 14:34:27.943771 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - kubemaster-pi I0714 14:34:27.943819 1 main.go:237] Installing signal handlers I0714 14:34:27.944064 1 main.go:352] Found network config - Backend type: vxlan I0714 14:34:27.944222 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0714 14:34:28.004675 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0714 14:34:28.004748 1 main.go:303] Running backend. I0714 14:34:28.004880 1 main.go:321] Waiting for all goroutines to exit I0714 14:34:28.004931 1 vxlan_network.go:56] watching for new subnet leases I0714 14:34:28.049933 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.050202 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.053918 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.054003 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.057332 1 iptables.go:136] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.061665 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.066452 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.069910 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.075067 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE I0714 14:34:28.078310 1 iptables.go:124] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.082389 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.098375 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.111379 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.122424 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE Calico Compile Calico for Raspberry PI WIP Deploy on Raspberry PI WIP Results WIP Reference Links Flannel Issue Flannel Issue2 Flannel Issue3","title":"Deploy Flannel"},{"location":"Kubernetes/DeployFlannel/#deploy-flannel-in-turing-pi","text":"In order to get the nodes and pods interface with each other accross the cluster. This post describes how I deployed Flannel acounting with the fact that some of the nodes have multiple interfaces (wlan0 and eth0).","title":"Deploy Flannel in Turing Pi"},{"location":"Kubernetes/DeployFlannel/#key-aspects","text":"Flannel seems to deploy ok. Looks like in trouble when multiple interfaces available Calico in not compiled by default for Rapsberry PI","title":"Key Aspects"},{"location":"Kubernetes/DeployFlannel/#flannel","text":"","title":"Flannel"},{"location":"Kubernetes/DeployFlannel/#setup-through-kubectl","text":"The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment cd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd kube-deployment/flannel/ kubectl apply -f flannel.yaml ### Flannel Issue 1: flannel.1. Link has incompatible address on master-pi, both the WLAN and LAN interfaces were activated. After unplugging the CAT5, behavior was similar. Moreover this had some impact on the kube-apiserver (see the number of restarts). ```bash $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m 1/1 Running 0 4h kube-system pod/coredns-78fcdf6894-cw5p8 1/1 Running 15 10d kube-system pod/coredns-78fcdf6894-czjcj 1/1 Running 15 10d kube-system pod/etcd-master-pi 1/1 Running 11 10d kube-system pod/kube-apiserver-master-pi 1/1 Running 599 10d kube-system pod/kube-controller-manager-master-pi 1/1 Running 38 10d kube-system pod/kube-flannel-ds-bhllh 1/1 Running 13 9d kube-system pod/kube-flannel-ds-q7cp2 0/1 CrashLoopBackOff 401 9d kube-system pod/kube-flannel-ds-wqxsz 1/1 Running 16 9d kube-system pod/kube-proxy-4chwh 1/1 Running 9 9d kube-system pod/kube-proxy-6r5mn 1/1 Running 5 9d kube-system pod/kube-proxy-vvj6j 1/1 Running 11 10d kube-system pod/kube-scheduler-master-pi 1/1 Running 13 10d kube-system pod/kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 20 7d kube-system pod/tiller-deploy-b59fcc885-66l7s 1/1 Running 0 6h $ kubectl logs pod/kube-flannel-ds-q7cp2 -n kube-system I0716 00:42:46.596796 1 main.go:474] Determining IP address of default interface I0716 00:42:46.598043 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:42:46.598138 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:42:46.775936 1 kube.go:283] Starting kube subnet manager I0716 00:42:46.775907 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:42:47.776280 1 kube.go:137] Node controller sync successful I0716 00:42:47.776400 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:42:47.776431 1 main.go:237] Installing signal handlers I0716 00:42:47.776697 1 main.go:352] Found network config - Backend type: vxlan I0716 00:42:47.776900 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0716 00:42:47.778884 1 main.go:279] Error registering network: failed to configure interface flannel.1: link has incompatible addresses. Remove additional addresses and try again.... I0716 00:42:47.778991 1 main.go:332] Stopping shutdownHandler... Deleting the pod, did not help. After recreation same issue reappeared. $ kubectl delete pod/kube-flannel-ds-q7cp2 -n kube-system $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 0/1 Error 1 17s Deleting the interface flannel.1 interface actually worked: $ sudo ip link delete flannel.1 $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 1/1 Running 5 3m $ kubectl logs pod/kube-flannel-ds-z7w4f -n kube-system I0716 00:52:14.555290 1 main.go:474] Determining IP address of default interface I0716 00:52:14.564490 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:52:14.564578 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:52:14.802491 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:52:14.802544 1 kube.go:283] Starting kube subnet manager I0716 00:52:15.803114 1 kube.go:137] Node controller sync successful I0716 00:52:15.803308 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:52:15.803909 1 main.go:237] Installing signal handlers I0716 00:52:15.804662 1 main.go:352] Found network config - Backend type: vxlan I0716 00:52:15.804985 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0716 00:52:15.875242 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0716 00:52:15.875367 1 main.go:303] Running backend. I0716 00:52:15.875489 1 main.go:321] Waiting for all goroutines to exit I0716 00:52:15.875559 1 vxlan_network.go:56] watching for new subnet leases","title":"Setup through kubectl"},{"location":"Kubernetes/DeployFlannel/#flannel-issue-2-multiple-interfaces","text":"Some of the PI have two interfaces running: wlan0 and eth0. The internal cluster network is using eth0. We need to force Flannel to use it.","title":"Flannel issue 2: Multiple interfaces"},{"location":"Kubernetes/DeployFlannel/#setup-through-kubectl_1","text":"Realize I was using v0.9.1 instead of v0.10.0. Let's update the file $ mkdir -p $HOME/kube-deployments/flannel $ cd $HOME/kube-deployments/flannel $ curl -sSL https://rawgit.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml | sed \"s/amd64/arm/g\" > flannel.yaml Let's add --iface=eth0 to the flanneld to in the flannel.yaml Let's update flannel from 0.9.1 to 0.10.0 at the same time we specify which interface to use. $ kubectl apply -f flannel.yaml clusterrole.rbac.authorization.k8s.io/flannel configured clusterrolebinding.rbac.authorization.k8s.io/flannel configured serviceaccount/flannel unchanged configmap/kube-flannel-cfg configured daemonset.extensions/kube-flannel-ds configured It seems it solved the flannel issue. The bug in kube 1.11.0 still there (restart of kube-apiserver) Will update to 1.11.1 when it is published $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-78fcdf6894-bn6wl 1/1 Running 0 6d kube-system pod/coredns-78fcdf6894-k52xb 1/1 Running 0 6d kube-system pod/etcd-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-controller-manager-kubemaster-pi 0/1 CrashLoopBackOff 1740 6d kube-system pod/kube-flannel-ds-62fz9 1/1 Running 984 6d kube-system pod/kube-flannel-ds-gwzdt 1/1 Running 0 6d kube-system pod/kube-flannel-ds-h7ln5 1/1 Running 0 6d kube-system pod/kube-flannel-ds-qs9lf 1/1 Running 0 6d kube-system pod/kube-flannel-ds-vwsjk 1/1 Running 0 6d kube-system pod/kube-proxy-45z5s 1/1 Running 0 6d kube-system pod/kube-proxy-4trsd 1/1 Running 0 6d kube-system pod/kube-proxy-ksj7c 1/1 Running 4 6d kube-system pod/kube-proxy-t7gmc 1/1 Running 0 6d kube-system pod/kube-proxy-tfmqb 1/1 Running 0 6d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 4 6d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 6d kube-system service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP 6d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 2 2 2 2 6d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-78fcdf6894 2 2 2 6d $ kubectl logs pod/kube-flannel-ds-62fz9 -n kube-system I0714 14:34:26.719081 1 main.go:474] Determining IP address of default interface I0714 14:34:26.728184 1 main.go:487] Using interface with name wlan0 and address 192.168.1.94 I0714 14:34:26.728273 1 main.go:504] Defaulting external address to interface address (192.168.1.94) I0714 14:34:26.942686 1 kube.go:283] Starting kube subnet manager I0714 14:34:26.943203 1 kube.go:130] Waiting 10m0s for node controller to sync I0714 14:34:27.943672 1 kube.go:137] Node controller sync successful I0714 14:34:27.943771 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - kubemaster-pi I0714 14:34:27.943819 1 main.go:237] Installing signal handlers I0714 14:34:27.944064 1 main.go:352] Found network config - Backend type: vxlan I0714 14:34:27.944222 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0714 14:34:28.004675 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0714 14:34:28.004748 1 main.go:303] Running backend. I0714 14:34:28.004880 1 main.go:321] Waiting for all goroutines to exit I0714 14:34:28.004931 1 vxlan_network.go:56] watching for new subnet leases I0714 14:34:28.049933 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.050202 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.053918 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.054003 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.057332 1 iptables.go:136] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.061665 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.066452 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.069910 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.075067 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE I0714 14:34:28.078310 1 iptables.go:124] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.082389 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.098375 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.111379 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.122424 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE","title":"Setup through kubectl"},{"location":"Kubernetes/DeployFlannel/#calico","text":"","title":"Calico"},{"location":"Kubernetes/DeployFlannel/#compile-calico-for-raspberry-pi","text":"WIP","title":"Compile Calico for Raspberry PI"},{"location":"Kubernetes/DeployFlannel/#deploy-on-raspberry-pi","text":"WIP","title":"Deploy on Raspberry PI"},{"location":"Kubernetes/DeployFlannel/#results","text":"WIP","title":"Results"},{"location":"Kubernetes/DeployFlannel/#reference-links","text":"Flannel Issue Flannel Issue2 Flannel Issue3","title":"Reference Links"},{"location":"Kubernetes/Installation/","text":"Kubernetes Installation Building Kubernetes on top of Turing Pi brings another dimension to the edge computing and learning, from setting up the OS, partitionning the OS, DHCP, NAT, cross compiling for the ARM32V7. Key Aspects Assemble a Turing Pi Cluster Deploy Kubernetes on Turing Pi Prepare control-plane and workers sudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | \\ sudo apt-key add - && echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" | \\ sudo tee /etc/apt/sources.list.d/kubernetes.list && sudo apt-get update -q sudo apt-get install -qy kubelet kubectl kubeadm To help during the initialization phase, get kubeadm to download the images onto docker kubeadm config images pull Initialize the Kubernetes control-plane node sudo kubeadm init --token-ttl=0 --pod-network-cidr=10.244.0.0/16 [init] using Kubernetes version: ... [preflight] running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.04.0-ce. Latest validated version: 18.06 [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [preflight] Activating the kubelet service [certs] Generated ca certificate and key. [certs] Generated apiserver-kubelet-client certificate and key. [certs] Generated apiserver certificate and key. [certs] apiserver serving cert is signed for DNS names [kubemaster-pi kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.2.1] [certs] Generated front-proxy-ca certificate and key. [certs] Generated front-proxy-client certificate and key. [certs] Generated etcd/ca certificate and key. [certs] Generated etcd/peer certificate and key. [certs] etcd/peer serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [192.168.2.1 127.0.0.1 ::1] [certs] Generated etcd/server certificate and key. [certs] etcd/server serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [127.0.0.1 ::1] [certs] Generated etcd/healthcheck-client certificate and key. [certs] Generated apiserver-etcd-client certificate and key. [certs] valid certificates and keys now exist in \"/etc/kubernetes/pki\" [certs] Generated sa key and public key. [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\" [controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" [controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" [controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" [etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\" [init] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\" [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 88.010108 seconds [uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubeletsin the cluster [mark-control-plane] Marking the node kubemaster-pi as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node kubemaster-pi as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"kubemaster-pi\" asan annotation [bootstraptoken] using token: vej1mx.6qf2xljr39rr1i14 [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx Initialize kubectl configuration As normal user (not root) mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes Add worker nodes kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx Setup Container Network Interface (CNI) At that point, nodes are not in ready state yet and CoreDNS in pending state kubectl get nodes kubectl get all -n kube-system The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml The nodes should turn ready and CoreDNS should start kubectl get nodes kubectl get all -n kube-system 5 node cluster kubectl get nodes NAME STATUS ROLES AGE VERSION kube-node01 Ready <none> 23d v1.9.8 kube-node02 Ready <none> 23d v1.9.8 kube-node03 Ready <none> 23d v1.9.8 kube-node04 Ready <none> 23d v1.9.8 kubemaster-pi Ready master 23d v1.9.8 kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/etcd-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-controller-manager-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-dns-7b6ff86f69-l7lf6 3/3 Running 0 23d kube-system pod/kube-flannel-ds-8xbx4 1/1 Running 0 23d kube-system pod/kube-flannel-ds-9cz9f 1/1 Running 0 23d kube-system pod/kube-flannel-ds-rgpcq 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xnjtz 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xxdf6 1/1 Running 0 23d kube-system pod/kube-proxy-5m95q 1/1 Running 0 23d kube-system pod/kube-proxy-7sh7m 1/1 Running 0 23d kube-system pod/kube-proxy-f7t9r 1/1 Running 0 23d kube-system pod/kube-proxy-pkqvd 1/1 Running 0 23d kube-system pod/kube-proxy-shrdr 1/1 Running 0 23d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 0 23d kube-system pod/kubernetes-dashboard-7fcc5cb979-8vbmp 1/1 Running 0 23d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP 23d kube-system service/kubernetes-dashboard NodePort 10.102.144.189 <none> 443:30383/TCP 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.extensions/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.extensions/kube-proxy 5 5 5 5 5 <none> 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.extensions/kube-dns 1 1 1 1 23d kube-system deployment.extensions/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.extensions/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.extensions/kubernetes-dashboard-7fcc5cb979 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 <none> 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/kube-dns 1 1 1 1 23d kube-system deployment.apps/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.apps/kubernetes-dashboard-7fcc5cb979 1 1 1 23d Cleanup Teardown cluster sudo kubeadm reset sudo docker rm $(sudo docker ps -qa) sudo docker image rm $(sudo docker image list -qa) sudo apt-get purge kubeadm kubectl kubelet sudo apt-get autoremove sudo rm -rf ~/.kube Reference Links Kubeadm on Hypriot","title":"Installation"},{"location":"Kubernetes/Installation/#kubernetes-installation","text":"Building Kubernetes on top of Turing Pi brings another dimension to the edge computing and learning, from setting up the OS, partitionning the OS, DHCP, NAT, cross compiling for the ARM32V7.","title":"Kubernetes Installation"},{"location":"Kubernetes/Installation/#key-aspects","text":"Assemble a Turing Pi Cluster Deploy Kubernetes on Turing Pi","title":"Key Aspects"},{"location":"Kubernetes/Installation/#prepare-control-plane-and-workers","text":"sudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | \\ sudo apt-key add - && echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" | \\ sudo tee /etc/apt/sources.list.d/kubernetes.list && sudo apt-get update -q sudo apt-get install -qy kubelet kubectl kubeadm To help during the initialization phase, get kubeadm to download the images onto docker kubeadm config images pull","title":"Prepare control-plane and workers"},{"location":"Kubernetes/Installation/#initialize-the-kubernetes-control-plane-node","text":"sudo kubeadm init --token-ttl=0 --pod-network-cidr=10.244.0.0/16 [init] using Kubernetes version: ... [preflight] running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.04.0-ce. Latest validated version: 18.06 [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [preflight] Activating the kubelet service [certs] Generated ca certificate and key. [certs] Generated apiserver-kubelet-client certificate and key. [certs] Generated apiserver certificate and key. [certs] apiserver serving cert is signed for DNS names [kubemaster-pi kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.2.1] [certs] Generated front-proxy-ca certificate and key. [certs] Generated front-proxy-client certificate and key. [certs] Generated etcd/ca certificate and key. [certs] Generated etcd/peer certificate and key. [certs] etcd/peer serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [192.168.2.1 127.0.0.1 ::1] [certs] Generated etcd/server certificate and key. [certs] etcd/server serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [127.0.0.1 ::1] [certs] Generated etcd/healthcheck-client certificate and key. [certs] Generated apiserver-etcd-client certificate and key. [certs] valid certificates and keys now exist in \"/etc/kubernetes/pki\" [certs] Generated sa key and public key. [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\" [controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" [controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" [controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" [etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\" [init] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\" [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 88.010108 seconds [uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubeletsin the cluster [mark-control-plane] Marking the node kubemaster-pi as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node kubemaster-pi as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"kubemaster-pi\" asan annotation [bootstraptoken] using token: vej1mx.6qf2xljr39rr1i14 [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx","title":"Initialize the Kubernetes control-plane node"},{"location":"Kubernetes/Installation/#initialize-kubectl-configuration","text":"As normal user (not root) mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes","title":"Initialize kubectl configuration"},{"location":"Kubernetes/Installation/#add-worker-nodes","text":"kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx","title":"Add worker nodes"},{"location":"Kubernetes/Installation/#setup-container-network-interface-cni","text":"At that point, nodes are not in ready state yet and CoreDNS in pending state kubectl get nodes kubectl get all -n kube-system The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml The nodes should turn ready and CoreDNS should start kubectl get nodes kubectl get all -n kube-system","title":"Setup Container Network Interface (CNI)"},{"location":"Kubernetes/Installation/#5-node-cluster","text":"kubectl get nodes NAME STATUS ROLES AGE VERSION kube-node01 Ready <none> 23d v1.9.8 kube-node02 Ready <none> 23d v1.9.8 kube-node03 Ready <none> 23d v1.9.8 kube-node04 Ready <none> 23d v1.9.8 kubemaster-pi Ready master 23d v1.9.8 kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/etcd-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-controller-manager-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-dns-7b6ff86f69-l7lf6 3/3 Running 0 23d kube-system pod/kube-flannel-ds-8xbx4 1/1 Running 0 23d kube-system pod/kube-flannel-ds-9cz9f 1/1 Running 0 23d kube-system pod/kube-flannel-ds-rgpcq 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xnjtz 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xxdf6 1/1 Running 0 23d kube-system pod/kube-proxy-5m95q 1/1 Running 0 23d kube-system pod/kube-proxy-7sh7m 1/1 Running 0 23d kube-system pod/kube-proxy-f7t9r 1/1 Running 0 23d kube-system pod/kube-proxy-pkqvd 1/1 Running 0 23d kube-system pod/kube-proxy-shrdr 1/1 Running 0 23d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 0 23d kube-system pod/kubernetes-dashboard-7fcc5cb979-8vbmp 1/1 Running 0 23d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP 23d kube-system service/kubernetes-dashboard NodePort 10.102.144.189 <none> 443:30383/TCP 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.extensions/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.extensions/kube-proxy 5 5 5 5 5 <none> 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.extensions/kube-dns 1 1 1 1 23d kube-system deployment.extensions/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.extensions/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.extensions/kubernetes-dashboard-7fcc5cb979 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 <none> 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/kube-dns 1 1 1 1 23d kube-system deployment.apps/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.apps/kubernetes-dashboard-7fcc5cb979 1 1 1 23d","title":"5 node cluster"},{"location":"Kubernetes/Installation/#cleanup","text":"Teardown cluster sudo kubeadm reset sudo docker rm $(sudo docker ps -qa) sudo docker image rm $(sudo docker image list -qa) sudo apt-get purge kubeadm kubectl kubelet sudo apt-get autoremove sudo rm -rf ~/.kube","title":"Cleanup"},{"location":"Kubernetes/Installation/#reference-links","text":"Kubeadm on Hypriot","title":"Reference Links"},{"location":"Kubernetes/K8SUpgrade/","text":"Upgrade tutorial Control plane node upgrade using kubeadm # apt-mark unhold kubeadm && \\ > apt-get update && apt-get install -y kubeadm && \\ > apt-mark hold kubeadm kubeadm was already not hold. Hit:2 http://raspbian.raspberrypi.org/raspbian stretch InRelease Hit:3 https://download.docker.com/linux/raspbian stretch InRelease Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease Hit:5 http://archive.raspberrypi.org/debian stretch InRelease Hit:4 https://packagecloud.io/Hypriot/rpi/debian stretch InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 38 not upgraded. Need to get 8,095 kB of archives. After this operation, 3,100 kB disk space will be freed. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubeadm armhf 1.12.0-00 [8,095 kB] Fetched 8,095 kB in 4s (1,962 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubeadm_1.12.0-00_armhf.deb ... Unpacking kubeadm (1.12.0-00) over (1.11.1-00) ... Setting up kubeadm (1.12.0-00) ... kubeadm set on hold. sudo kubeadm upgrade plan [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/versions] Latest stable version: v1.12.0 [upgrade/versions] Latest version in the v1.11 series: v1.11.3 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.11.3 Upgrade to the latest version in the v1.11 series: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.11.3 Controller Manager v1.11.0 v1.11.3 Scheduler v1.11.0 v1.11.3 Kube Proxy v1.11.0 v1.11.3 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.18 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.11.3 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.12.0 Upgrade to the latest stable version: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.12.0 Controller Manager v1.11.0 v1.12.0 Scheduler v1.11.0 v1.12.0 Kube Proxy v1.11.0 v1.12.0 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.24 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.12.0 _____________________________________________________________________ $ sudo kubeadm upgrade apply v1.12.0 [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file. [upgrade/version] You have chosen to change the cluster version to \"v1.12.0\" [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [upgrade/prepull] Prepulling image for component kube-scheduler. [upgrade/prepull] Prepulling image for component etcd. [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.12.0\"... Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f [etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/etcd.yaml\" [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/etcd.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 77c6076a4d6ee044b744b041125cf918 [apiclient] Found 1 Pods for label selector component=etcd [upgrade/staticpods] Component \"etcd\" upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [util/etcd] Waiting 0s for initial delay [util/etcd] Attempting to see if all cluster endpoints are available 1/10 [upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315\" [controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-apiserver.yaml\" [controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-controller-manager.yaml\" [controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-scheduler.yaml\" [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-apiserver.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-apiserver-master-pi hash: a92106d6db4c8b5835a47f5f56c33fdb [apiclient] Found 1 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully! [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-controller-manager.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-controller-manager-master-pi hash: 980b4156606df8caafd0ad8abacc1485 [apiclient] Found 1 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully! [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-scheduler.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: kube-scheduler-master-pi hash: 1b5ec5be325bf29f60be62789416a99e [apiclient] Found 1 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully! [uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubelets in the cluster [kubelet] Downloading configuration for the kubelet from the \"kubelet-config-1.12\" ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"master-pi\" as an annotation [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.12.0\". Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. Kubernetes servers are running v1.12 $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"11\", GitVersion:\"v1.11.1\", GitCommit:\"b1b29978270dc22fecc592ac55d903350454310a\", GitTreeState:\"clean\", BuildDate:\"2018-07-17T18:53:20Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/arm\"} Server Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.0\", GitCommit:\"0ed33881dc4355495f623c6f22e7dd0b7632b7c0\", GitTreeState:\"clean\", BuildDate:\"2018-09-27T16:55:41Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/arm\"} my kubelets are still running kubernetes v1.11.1 $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready <none> 86d v1.11.1 master-pi Ready master 86d v1.11.1 nas-pi Ready <none> 85d v1.11.1 Upgrade kubectl Install newer version of kubectl using apt-get $ sudo apt-get install kubectl Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubectl 1 upgraded, 0 newly installed, 0 to remove and 37 not upgraded. Need to get 8,639 kB of archives. After this operation, 1,783 kB of additional disk space will be used. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubectl armhf 1.12.0-00 [8,639 kB] Fetched 8,639 kB in 6s (1,270 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubectl_1.12.0-00_armhf.deb ... Unpacking kubectl (1.12.0-00) over (1.11.1-00) ... Setting up kubectl (1.12.0-00) ... Check that kubectl is now 1.12 $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.0\", GitCommit:\"0ed33881dc4355495f623c6f22e7dd0b7632b7c0\", GitTreeState:\"clean\", BuildDate:\"2018-09-27T17:05:32Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/arm\"} Server Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.0\", GitCommit:\"0ed33881dc4355495f623c6f22e7dd0b7632b7c0\", GitTreeState:\"clean\", BuildDate:\"2018-09-27T16:55:41Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/arm\"} Upgrade kubelet on control plane node $ sudo apt-get upgrade -y kubelet Reading package lists... Done Building dependency tree Reading state information... Done Calculating upgrade... Done The following packages will be upgraded: ... $ sudo systemctl restart kubelet $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready <none> 86d v1.11.1 master-pi Ready master 86d v1.12.0 nas-pi Ready <none> 85d v1.11.1 Looks that as useual something is wrong with flannel CNI $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-6gvf6 1/1 Running 0 60d kube-system pod/coredns-576cbf47c7-nzhfj 1/1 Running 0 37m kube-system pod/coredns-576cbf47c7-wkmhr 1/1 Running 0 37m kube-system pod/etcd-master-pi 1/1 Running 0 3m43s kube-system pod/kube-apiserver-master-pi 1/1 Running 2 3m43s kube-system pod/kube-controller-manager-master-pi 1/1 Running 1 3m43s kube-system pod/kube-flannel-ds-4495g 1/1 Running 4 75d kube-system pod/kube-flannel-ds-52ssk 0/1 Error 10 75d kube-system pod/kube-flannel-ds-gj65n 1/1 Running 4 75d Investigation shows: ```bash $ kubectl logs pod/kube-flannel-ds-52ssk -n kube-system I0930 01:34:13.768925 1 main.go:475] Determining IP address of default interface I0930 01:34:13.778622 1 main.go:488] Using interface with name wlan0 and address 192.168.1.95 I0930 01:34:13.778716 1 main.go:505] Defaulting external address to interface address (192.168.1.95) I0930 01:34:14.168318 1 kube.go:131] Waiting 10m0s for node controller to sync I0930 01:34:14.168890 1 kube.go:294] Starting kube subnet manager I0930 01:34:15.169929 1 kube.go:138] Node controller sync successful I0930 01:34:15.170035 1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - master-pi I0930 01:34:15.170064 1 main.go:238] Installing signal handlers I0930 01:34:15.170415 1 main.go:353] Found network config - Backend type: vxlan I0930 01:34:15.170797 1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0930 01:34:15.256843 1 main.go:280] Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. &netlink.Vxlan{LinkAttrs:netlink.LinkAttrs{Index:5, MTU:1450, TxQLen:0, Name:\"flannel.1\", HardwareAddr:net.HardwareAddr{0x26, 0xd5, 0xd0, 0xdd, 0x56, 0x1a}, ... Let's apply usual receipe ``bash sudo ip link delete flannel.1 Brute force fix seems to have done the fix....This is lucky we don't have production traffic on that node. ```bash $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 51m coredns-576cbf47c7-wkmhr 1/1 Running 0 51m etcd-master-pi 1/1 Running 0 17m kube-apiserver-master-pi 1/1 Running 2 17m kube-controller-manager-master-pi 1/1 Running 1 17m kube-flannel-ds-4495g 1/1 Running 4 75d kube-flannel-ds-52ssk 1/1 Running 13 75d kube-flannel-ds-gj65n 1/1 Running 4 75d kube-proxy-c2264 1/1 Running 0 51m kube-proxy-snjsg 1/1 Running 0 49m kube-proxy-zgqjb 1/1 Running 1 50m kube-scheduler-master-pi 1/1 Running 1 17m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d Update first worker node sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet Same flannel issue $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 87m coredns-576cbf47c7-wkmhr 1/1 Running 1 87m etcd-master-pi 1/1 Running 0 53m kube-apiserver-master-pi 1/1 Running 2 53m kube-controller-manager-master-pi 1/1 Running 1 53m kube-flannel-ds-4495g 0/1 CrashLoopBackOff 10 75d ... same hack to fix the issue sudo ip link delete flannel.1 same hack seems to be effective $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 111m coredns-576cbf47c7-wkmhr 1/1 Running 1 111m etcd-master-pi 1/1 Running 0 77m kube-apiserver-master-pi 1/1 Running 2 77m kube-controller-manager-master-pi 1/1 Running 1 77m kube-flannel-ds-4495g 1/1 Running 11 75d ... Other worker node sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet sudo ip link delete flannel.1 Check consistency of the cluster $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 1 143m coredns-576cbf47c7-wkmhr 1/1 Running 1 143m etcd-master-pi 1/1 Running 0 109m kube-apiserver-master-pi 1/1 Running 2 109m kube-controller-manager-master-pi 1/1 Running 1 109m kube-flannel-ds-4495g 1/1 Running 11 76d kube-flannel-ds-52ssk 1/1 Running 13 76d kube-flannel-ds-gj65n 1/1 Running 13 76d kube-proxy-c2264 1/1 Running 1 143m kube-proxy-snjsg 1/1 Running 1 141m kube-proxy-zgqjb 1/1 Running 1 142m kube-scheduler-master-pi 1/1 Running 1 109m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready <none> 86d v1.12.0 master-pi Ready master 86d v1.12.0 nas-pi Ready <none> 86d v1.12.0 Conclusion At a glance, the cluster seems to be healthy I still need to find sometool like sonobuoy to validate that the cluster is healthy Reference Links Official Upgrade documentation","title":"K8S Upgrade"},{"location":"Kubernetes/K8SUpgrade/#upgrade-tutorial","text":"","title":"Upgrade tutorial"},{"location":"Kubernetes/K8SUpgrade/#control-plane-node-upgrade-using-kubeadm","text":"# apt-mark unhold kubeadm && \\ > apt-get update && apt-get install -y kubeadm && \\ > apt-mark hold kubeadm kubeadm was already not hold. Hit:2 http://raspbian.raspberrypi.org/raspbian stretch InRelease Hit:3 https://download.docker.com/linux/raspbian stretch InRelease Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease Hit:5 http://archive.raspberrypi.org/debian stretch InRelease Hit:4 https://packagecloud.io/Hypriot/rpi/debian stretch InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 38 not upgraded. Need to get 8,095 kB of archives. After this operation, 3,100 kB disk space will be freed. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubeadm armhf 1.12.0-00 [8,095 kB] Fetched 8,095 kB in 4s (1,962 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubeadm_1.12.0-00_armhf.deb ... Unpacking kubeadm (1.12.0-00) over (1.11.1-00) ... Setting up kubeadm (1.12.0-00) ... kubeadm set on hold. sudo kubeadm upgrade plan [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/versions] Latest stable version: v1.12.0 [upgrade/versions] Latest version in the v1.11 series: v1.11.3 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.11.3 Upgrade to the latest version in the v1.11 series: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.11.3 Controller Manager v1.11.0 v1.11.3 Scheduler v1.11.0 v1.11.3 Kube Proxy v1.11.0 v1.11.3 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.18 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.11.3 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.12.0 Upgrade to the latest stable version: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.12.0 Controller Manager v1.11.0 v1.12.0 Scheduler v1.11.0 v1.12.0 Kube Proxy v1.11.0 v1.12.0 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.24 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.12.0 _____________________________________________________________________ $ sudo kubeadm upgrade apply v1.12.0 [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file. [upgrade/version] You have chosen to change the cluster version to \"v1.12.0\" [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [upgrade/prepull] Prepulling image for component kube-scheduler. [upgrade/prepull] Prepulling image for component etcd. [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.12.0\"... Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f [etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/etcd.yaml\" [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/etcd.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 77c6076a4d6ee044b744b041125cf918 [apiclient] Found 1 Pods for label selector component=etcd [upgrade/staticpods] Component \"etcd\" upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [util/etcd] Waiting 0s for initial delay [util/etcd] Attempting to see if all cluster endpoints are available 1/10 [upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315\" [controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-apiserver.yaml\" [controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-controller-manager.yaml\" [controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-scheduler.yaml\" [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-apiserver.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-apiserver-master-pi hash: a92106d6db4c8b5835a47f5f56c33fdb [apiclient] Found 1 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully! [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-controller-manager.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-controller-manager-master-pi hash: 980b4156606df8caafd0ad8abacc1485 [apiclient] Found 1 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully! [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-scheduler.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: kube-scheduler-master-pi hash: 1b5ec5be325bf29f60be62789416a99e [apiclient] Found 1 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully! [uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubelets in the cluster [kubelet] Downloading configuration for the kubelet from the \"kubelet-config-1.12\" ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"master-pi\" as an annotation [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.12.0\". Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. Kubernetes servers are running v1.12 $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"11\", GitVersion:\"v1.11.1\", GitCommit:\"b1b29978270dc22fecc592ac55d903350454310a\", GitTreeState:\"clean\", BuildDate:\"2018-07-17T18:53:20Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/arm\"} Server Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.0\", GitCommit:\"0ed33881dc4355495f623c6f22e7dd0b7632b7c0\", GitTreeState:\"clean\", BuildDate:\"2018-09-27T16:55:41Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/arm\"} my kubelets are still running kubernetes v1.11.1 $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready <none> 86d v1.11.1 master-pi Ready master 86d v1.11.1 nas-pi Ready <none> 85d v1.11.1","title":"Control plane node upgrade using kubeadm"},{"location":"Kubernetes/K8SUpgrade/#upgrade-kubectl","text":"Install newer version of kubectl using apt-get $ sudo apt-get install kubectl Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubectl 1 upgraded, 0 newly installed, 0 to remove and 37 not upgraded. Need to get 8,639 kB of archives. After this operation, 1,783 kB of additional disk space will be used. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubectl armhf 1.12.0-00 [8,639 kB] Fetched 8,639 kB in 6s (1,270 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubectl_1.12.0-00_armhf.deb ... Unpacking kubectl (1.12.0-00) over (1.11.1-00) ... Setting up kubectl (1.12.0-00) ... Check that kubectl is now 1.12 $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.0\", GitCommit:\"0ed33881dc4355495f623c6f22e7dd0b7632b7c0\", GitTreeState:\"clean\", BuildDate:\"2018-09-27T17:05:32Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/arm\"} Server Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.0\", GitCommit:\"0ed33881dc4355495f623c6f22e7dd0b7632b7c0\", GitTreeState:\"clean\", BuildDate:\"2018-09-27T16:55:41Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/arm\"}","title":"Upgrade kubectl"},{"location":"Kubernetes/K8SUpgrade/#upgrade-kubelet-on-control-plane-node","text":"$ sudo apt-get upgrade -y kubelet Reading package lists... Done Building dependency tree Reading state information... Done Calculating upgrade... Done The following packages will be upgraded: ... $ sudo systemctl restart kubelet $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready <none> 86d v1.11.1 master-pi Ready master 86d v1.12.0 nas-pi Ready <none> 85d v1.11.1 Looks that as useual something is wrong with flannel CNI $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-6gvf6 1/1 Running 0 60d kube-system pod/coredns-576cbf47c7-nzhfj 1/1 Running 0 37m kube-system pod/coredns-576cbf47c7-wkmhr 1/1 Running 0 37m kube-system pod/etcd-master-pi 1/1 Running 0 3m43s kube-system pod/kube-apiserver-master-pi 1/1 Running 2 3m43s kube-system pod/kube-controller-manager-master-pi 1/1 Running 1 3m43s kube-system pod/kube-flannel-ds-4495g 1/1 Running 4 75d kube-system pod/kube-flannel-ds-52ssk 0/1 Error 10 75d kube-system pod/kube-flannel-ds-gj65n 1/1 Running 4 75d Investigation shows: ```bash $ kubectl logs pod/kube-flannel-ds-52ssk -n kube-system I0930 01:34:13.768925 1 main.go:475] Determining IP address of default interface I0930 01:34:13.778622 1 main.go:488] Using interface with name wlan0 and address 192.168.1.95 I0930 01:34:13.778716 1 main.go:505] Defaulting external address to interface address (192.168.1.95) I0930 01:34:14.168318 1 kube.go:131] Waiting 10m0s for node controller to sync I0930 01:34:14.168890 1 kube.go:294] Starting kube subnet manager I0930 01:34:15.169929 1 kube.go:138] Node controller sync successful I0930 01:34:15.170035 1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - master-pi I0930 01:34:15.170064 1 main.go:238] Installing signal handlers I0930 01:34:15.170415 1 main.go:353] Found network config - Backend type: vxlan I0930 01:34:15.170797 1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0930 01:34:15.256843 1 main.go:280] Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. &netlink.Vxlan{LinkAttrs:netlink.LinkAttrs{Index:5, MTU:1450, TxQLen:0, Name:\"flannel.1\", HardwareAddr:net.HardwareAddr{0x26, 0xd5, 0xd0, 0xdd, 0x56, 0x1a}, ... Let's apply usual receipe ``bash sudo ip link delete flannel.1 Brute force fix seems to have done the fix....This is lucky we don't have production traffic on that node. ```bash $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 51m coredns-576cbf47c7-wkmhr 1/1 Running 0 51m etcd-master-pi 1/1 Running 0 17m kube-apiserver-master-pi 1/1 Running 2 17m kube-controller-manager-master-pi 1/1 Running 1 17m kube-flannel-ds-4495g 1/1 Running 4 75d kube-flannel-ds-52ssk 1/1 Running 13 75d kube-flannel-ds-gj65n 1/1 Running 4 75d kube-proxy-c2264 1/1 Running 0 51m kube-proxy-snjsg 1/1 Running 0 49m kube-proxy-zgqjb 1/1 Running 1 50m kube-scheduler-master-pi 1/1 Running 1 17m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d","title":"Upgrade kubelet on control plane node"},{"location":"Kubernetes/K8SUpgrade/#update-first-worker-node","text":"sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet Same flannel issue $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 87m coredns-576cbf47c7-wkmhr 1/1 Running 1 87m etcd-master-pi 1/1 Running 0 53m kube-apiserver-master-pi 1/1 Running 2 53m kube-controller-manager-master-pi 1/1 Running 1 53m kube-flannel-ds-4495g 0/1 CrashLoopBackOff 10 75d ... same hack to fix the issue sudo ip link delete flannel.1 same hack seems to be effective $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 111m coredns-576cbf47c7-wkmhr 1/1 Running 1 111m etcd-master-pi 1/1 Running 0 77m kube-apiserver-master-pi 1/1 Running 2 77m kube-controller-manager-master-pi 1/1 Running 1 77m kube-flannel-ds-4495g 1/1 Running 11 75d ...","title":"Update first worker node"},{"location":"Kubernetes/K8SUpgrade/#other-worker-node","text":"sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet sudo ip link delete flannel.1","title":"Other worker node"},{"location":"Kubernetes/K8SUpgrade/#check-consistency-of-the-cluster","text":"$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 1 143m coredns-576cbf47c7-wkmhr 1/1 Running 1 143m etcd-master-pi 1/1 Running 0 109m kube-apiserver-master-pi 1/1 Running 2 109m kube-controller-manager-master-pi 1/1 Running 1 109m kube-flannel-ds-4495g 1/1 Running 11 76d kube-flannel-ds-52ssk 1/1 Running 13 76d kube-flannel-ds-gj65n 1/1 Running 13 76d kube-proxy-c2264 1/1 Running 1 143m kube-proxy-snjsg 1/1 Running 1 141m kube-proxy-zgqjb 1/1 Running 1 142m kube-scheduler-master-pi 1/1 Running 1 109m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready <none> 86d v1.12.0 master-pi Ready master 86d v1.12.0 nas-pi Ready <none> 86d v1.12.0","title":"Check consistency of the cluster"},{"location":"Kubernetes/K8SUpgrade/#conclusion","text":"At a glance, the cluster seems to be healthy I still need to find sometool like sonobuoy to validate that the cluster is healthy","title":"Conclusion"},{"location":"Kubernetes/K8SUpgrade/#reference-links","text":"Official Upgrade documentation","title":"Reference Links"},{"location":"Kubernetes/PersistentVolume/","text":"Persistent Volume In order to install Prometheus, NATS, Cassandra using Kubernetes, we need to first create Persistency Volumes Procedures The procedure assumes that you have 32G SD Card that you can partition into one 16G disk and seven 7G disks. Partition the SD cards Use the Rescue dongle to create the partition. The ideal solution would be to update the cloud-init of the HypriotOS to do that automatically. Mount the disks Mounting the partitions, involves creating new directories on each node and adding \".mount\" files under systemd This kind of done automatically under by the mount-disks playbook cd $HOME/mgt/ ansible picluster -i inventory/ -m shell -a \"df -kh\" ansible-playbook -i inventory/ playbooks/mount_disks.yaml ansible picluster -i inventory/ -m shell -a \"df -kh\" Create the Persistency Volumes in Kubernetes The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment Because we did not convert the go code to create the volumes automatically from the /mnt/disks/..., we have to kind of create the PV by hand. cd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd $HOME/kube-deployment/local-storage kubectl apply -f local-storageclass.yaml kubectl apply -f admin_account.yaml kubectl apply -f kubemaster-pi-volumes.yaml kubectl apply -f kube-node01-volumes.yaml kubectl apply -f kube-node02-volumes.yaml kubectl apply -f kube-node03-volumes.yaml kubectl apply -f kube-node04-volumes.yaml Results Reference Links TBD","title":"Persistent Volume"},{"location":"Kubernetes/PersistentVolume/#persistent-volume","text":"In order to install Prometheus, NATS, Cassandra using Kubernetes, we need to first create Persistency Volumes","title":"Persistent Volume"},{"location":"Kubernetes/PersistentVolume/#procedures","text":"The procedure assumes that you have 32G SD Card that you can partition into one 16G disk and seven 7G disks.","title":"Procedures"},{"location":"Kubernetes/PersistentVolume/#partition-the-sd-cards","text":"Use the Rescue dongle to create the partition. The ideal solution would be to update the cloud-init of the HypriotOS to do that automatically.","title":"Partition the SD cards"},{"location":"Kubernetes/PersistentVolume/#mount-the-disks","text":"Mounting the partitions, involves creating new directories on each node and adding \".mount\" files under systemd This kind of done automatically under by the mount-disks playbook cd $HOME/mgt/ ansible picluster -i inventory/ -m shell -a \"df -kh\" ansible-playbook -i inventory/ playbooks/mount_disks.yaml ansible picluster -i inventory/ -m shell -a \"df -kh\"","title":"Mount the disks"},{"location":"Kubernetes/PersistentVolume/#create-the-persistency-volumes-in-kubernetes","text":"The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment Because we did not convert the go code to create the volumes automatically from the /mnt/disks/..., we have to kind of create the PV by hand. cd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd $HOME/kube-deployment/local-storage kubectl apply -f local-storageclass.yaml kubectl apply -f admin_account.yaml kubectl apply -f kubemaster-pi-volumes.yaml kubectl apply -f kube-node01-volumes.yaml kubectl apply -f kube-node02-volumes.yaml kubectl apply -f kube-node03-volumes.yaml kubectl apply -f kube-node04-volumes.yaml","title":"Create the Persistency Volumes in Kubernetes"},{"location":"Kubernetes/PersistentVolume/#results","text":"","title":"Results"},{"location":"Kubernetes/PersistentVolume/#reference-links","text":"TBD","title":"Reference Links"},{"location":"hardware/","text":"Core Turing Pi is a private mobile cloud in a mini ITX form factor. It\u2019s a scale model of bare metal clusters like you see in data centers. Turing Pi cluster board can scale up to 7 compute nodes. A Turing Pi is made up of two core elements: Turing Pi cluster board specifications Raspberry Pi Compute Modules Compute Modules Supported Compute Modules Now let's get you up and running. Hardware Software Full Documentation Discord Channel #tech-support","title":"Core"},{"location":"hardware/#core","text":"Turing Pi is a private mobile cloud in a mini ITX form factor. It\u2019s a scale model of bare metal clusters like you see in data centers. Turing Pi cluster board can scale up to 7 compute nodes. A Turing Pi is made up of two core elements:","title":"Core"},{"location":"hardware/#turing-pi-cluster-board","text":"specifications","title":"Turing Pi cluster board"},{"location":"hardware/#raspberry-pi-compute-modules","text":"Compute Modules Supported Compute Modules Now let's get you up and running. Hardware Software Full Documentation Discord Channel #tech-support","title":"Raspberry Pi Compute Modules"},{"location":"software/","text":"Build Options Depending on your specific needs, you might choose to build a Home Lab or a machine to learn Cloud-Native Technologies with your Turing Pi. With Turing Pi there are options. Here are the most common use-cases, level of difficulty and appropriate operating systems and or modifications. Not sure what opperating system you need? Review some real world use cases and get inspired . Home Lab Home Lab description here. Basic A basic Home Lab setup with this OS does these types of tasks very well. Moderate A moderate Home Lab setup will do that and this using this OS and these modifications. Advanced An advanced Home Lab and a basic Native-Cloud set are very similar but using this OS and these modifications you will be lean and mean. Cloud-Native Cloud-Native description here. Basic A basic Cloud-Native system up for this and this tasks, we are going to need this OS . Moderate If basic just won't do and you ready for some heavy lifting task or task. The OS will need to be modified with these additions Advanced Go BIG with OS and these additions . Hardware Software Full Documentation Discord Channel #tech-support","title":"Build Options"},{"location":"software/#build-options","text":"Depending on your specific needs, you might choose to build a Home Lab or a machine to learn Cloud-Native Technologies with your Turing Pi. With Turing Pi there are options. Here are the most common use-cases, level of difficulty and appropriate operating systems and or modifications.","title":"Build Options"},{"location":"software/#not-sure-what-opperating-system-you-need","text":"Review some real world use cases and get inspired .","title":"Not sure what opperating system you need?"},{"location":"software/#home-lab","text":"Home Lab description here.","title":"Home Lab"},{"location":"software/#basic","text":"A basic Home Lab setup with this OS does these types of tasks very well.","title":"Basic"},{"location":"software/#moderate","text":"A moderate Home Lab setup will do that and this using this OS and these modifications.","title":"Moderate"},{"location":"software/#advanced","text":"An advanced Home Lab and a basic Native-Cloud set are very similar but using this OS and these modifications you will be lean and mean.","title":"Advanced"},{"location":"software/#cloud-native","text":"Cloud-Native description here.","title":"Cloud-Native"},{"location":"software/#basic_1","text":"A basic Cloud-Native system up for this and this tasks, we are going to need this OS .","title":"Basic"},{"location":"software/#moderate_1","text":"If basic just won't do and you ready for some heavy lifting task or task. The OS will need to be modified with these additions","title":"Moderate"},{"location":"software/#advanced_1","text":"Go BIG with OS and these additions . Hardware Software Full Documentation Discord Channel #tech-support","title":"Advanced"},{"location":"use-cases/","text":"Solutions These Turning Pis have been seen in the wild: Home Lab Edge Apps Kubernetes another and another Have you created a unique Turing Pi solution? Inspire others by posting your build on the Discord Channel #use-cases . We might ask to feature your talant and hard work on this Wiki!","title":"Solutions"},{"location":"use-cases/#solutions","text":"These Turning Pis have been seen in the wild: Home Lab Edge Apps Kubernetes another and another","title":"Solutions"},{"location":"use-cases/#have-you-created-a-unique-turing-pi-solution","text":"Inspire others by posting your build on the Discord Channel #use-cases . We might ask to feature your talant and hard work on this Wiki!","title":"Have you created a unique Turing Pi solution?"},{"location":"use-cases/home-lab/","text":"Toni's Home Lab Here is Toni's Turing Pi Home Lab build. Motivation for build. Strugles and Triumphs during the build process. Hardware Turing Pi Cluster Board 7 Raspberry Pi 3+ compute Modules 8bg each Case Power Supply Software software software Take Aways here Bonus here Thank You here Have you created a unique Turing Pi solution? Inspire others by posting your build on the Discord Channel #use-cases . We might ask to feature your talant and hard work on this Wiki!","title":"Toni's Home Lab"},{"location":"use-cases/home-lab/#tonis-home-lab","text":"Here is Toni's Turing Pi Home Lab build. Motivation for build. Strugles and Triumphs during the build process.","title":"Toni's Home Lab"},{"location":"use-cases/home-lab/#hardware","text":"Turing Pi Cluster Board 7 Raspberry Pi 3+ compute Modules 8bg each Case Power Supply","title":"Hardware"},{"location":"use-cases/home-lab/#software","text":"software software","title":"Software"},{"location":"use-cases/home-lab/#take-aways","text":"here","title":"Take Aways"},{"location":"use-cases/home-lab/#bonus","text":"here","title":"Bonus"},{"location":"use-cases/home-lab/#thank-you","text":"here","title":"Thank You"},{"location":"use-cases/home-lab/#have-you-created-a-unique-turing-pi-solution","text":"Inspire others by posting your build on the Discord Channel #use-cases . We might ask to feature your talant and hard work on this Wiki!","title":"Have you created a unique Turing Pi solution?"}]}